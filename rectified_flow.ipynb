{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "from modules.networks.Unet import ContextUnet\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Use only the first GPU\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "tf = transforms.Compose([transforms.ToTensor()]) # mnist is already normalised 0 to 1\n",
    "val_dataset = MNIST(\"/data/edherron/data/MNIST\", train=False, download=False, transform=tf)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, \n",
    "                                        batch_size=256, \n",
    "                                        shuffle=True, \n",
    "                                        drop_last=True, \n",
    "                                        num_workers=1\n",
    "                                        )\n",
    "\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dim=32):\n",
    "        super().__init__()\n",
    "        # Initial convolution block\n",
    "        layers = [nn.Conv2d(1, dim, 4), nn.InstanceNorm2d(dim * 2), nn.Tanh()]\n",
    "        # Downsampling\n",
    "        for _ in range(4):\n",
    "            layers += [nn.Conv2d(dim, dim * 2, 4), nn.InstanceNorm2d(dim * 2), nn.Tanh()]\n",
    "            dim *= 2\n",
    "        # Final block with 1D convolution for demonstration purposes\n",
    "        layers += [nn.Conv2d(dim, 6, 1), nn.InstanceNorm2d(1), nn.Tanh() ]\n",
    "        self.model_blocks = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model_blocks(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dim=32):\n",
    "        super().__init__()\n",
    "        dim = dim * 2 ** 3\n",
    "        layers = [nn.Conv2d(6,dim,1), nn.InstanceNorm2d(dim), nn.LeakyReLU(0.2, inplace=True)]\n",
    "        # Upsampling\n",
    "        for _ in range(5):\n",
    "            layers += [nn.ConvTranspose2d(dim, dim // 2, 4), nn.InstanceNorm2d(dim // 2), nn.LeakyReLU(0.2, inplace=True)]\n",
    "            dim = dim // 2\n",
    "        # Output layer\n",
    "        layers += [nn.Conv2d(dim, 1, 1)]\n",
    "        self.model_blocks = nn.Sequential(*layers, nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model_blocks(x)\n",
    "        return x\n",
    "    \n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        self.ts = nn.Linear(1, 6)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        latent = self.encoder(x)\n",
    "        ts = repeat(self.ts(t), 'b v -> b v 1 1')\n",
    "        eps = self.decoder(latent + ts)\n",
    "        return eps\n",
    "    \n",
    "def pad(tensor):\n",
    "    return repeat(tensor, 'b 1 -> b 1 1 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RectifiedFlow():\n",
    "    def __init__(self, model=None, device=None, num_steps=1000):\n",
    "        self.model = Network()\n",
    "        self.model.to(device)\n",
    "        self.N = num_steps\n",
    "        self.device = device\n",
    "        \n",
    "    def get_train_tuple(self, z0=None, z1=None):\n",
    "        t = torch.rand((z1.shape[0],1)).to(self.device)\n",
    "        z_t = pad(t) * z1 + (1. - pad(t)) * z0\n",
    "        target = z1 - z0\n",
    "        return z_t, t, target\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sample_ode(self, z0=None, N=None):\n",
    "        if N is None:\n",
    "            N = self.N\n",
    "        dt = 1./N\n",
    "        trajectory = []\n",
    "        z = z0.detach().clone().to(self.device)\n",
    "        \n",
    "        \n",
    "        trajectory.append(z.detach().clone())\n",
    "        for i in range(N):\n",
    "            t = torch.ones((z.shape[0],)) * i / N\n",
    "            t = t.to(self.device)\n",
    "            pred = self.model(z, t)\n",
    "            z = z.detach().clone() + pred * dt\n",
    "            \n",
    "            trajectory.append(z.detach().clone())\n",
    "        return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rectified_flow(data_loader, rectified_flow, opt, device):\n",
    "    rectified_flow.model.train()\n",
    "    running_loss = 0.0\n",
    "    for data in data_loader:\n",
    "        z1, _ = data\n",
    "        z1 = z1.to(device)\n",
    "        # z1 = rearrange(z1.to(device), 'b c h w -> b (c h w)')\n",
    "        z0 = torch.randn_like(z1).to(device)\n",
    "        \n",
    "        z_t, t, target = rectified_flow.get_train_tuple(z0, z1)\n",
    "        \n",
    "        pred = rectified_flow.model(z_t, t)\n",
    "        \n",
    "        loss = F.mse_loss(pred, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        running_loss += loss.item()\n",
    "    avg_loss = running_loss / len(data_loader)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters:  3490291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]/home/microway/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n",
      "  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n",
      " 20%|██        | 1/5 [00:05<00:20,  5.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss from epoch  0 :  1.3196749075865135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:09<00:14,  4.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss from epoch  1 :  1.3210092935806665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:15<00:10,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss from epoch  2 :  1.3206556362983508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:20<00:05,  5.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss from epoch  3 :  1.321506808965634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:25<00:00,  5.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss from epoch  4 :  1.3215078971324823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Init all of our models\n",
    "model = Network()\n",
    "RF = RectifiedFlow(model, device)\n",
    "\n",
    "print(\"Number of parameters: \", sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "for i in tqdm(range(5)):\n",
    "    loss_rec = train_rectified_flow(val_loader, RF, opt, device)\n",
    "    print('loss from epoch ', i, ': ', loss_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.randn((1,28**2))\n",
    "trajectory = RF.sample_ode(z0 = z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(trajectory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(trajectory))\n",
    "print(type(trajectory[69]))\n",
    "print(trajectory[69].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `data_list` is your list of tensors\n",
    "fig, axs = plt.subplots(1, 11, figsize=(20, 2))  # Adjust figsize as needed\n",
    "\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    idx = i * 100  # Every 100th element\n",
    "    if idx < len(trajectory):\n",
    "        img = trajectory[idx].reshape(28, 28).detach().cpu().numpy()  # Reshape tensor to 28x28 for visualization\n",
    "        ax.imshow(img, cmap='gray')  # Plot as grayscale image\n",
    "        ax.set_title(f'Index {idx}')\n",
    "        ax.axis('off')\n",
    "    else:\n",
    "        ax.axis('off')  # Hide axes for plots beyond the list length\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(data_loader, ae, opt, device):\n",
    "    ae.train()\n",
    "    running_loss = 0.0\n",
    "    for data in data_loader:\n",
    "        images, _ = data\n",
    "        images = rearrange(images.to(device), 'b c h w -> b (c h w)')\n",
    "        recon = ae(images)\n",
    "        loss = F.mse_loss(recon, images)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        running_loss += loss.item()\n",
    "    avg_loss = running_loss / len(data_loader)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init all of our models\n",
    "ae = MLP_AE()\n",
    "ae.to(device)\n",
    "print(\"Number of parameters: \", sum(p.numel() for p in ae.parameters()))\n",
    "\n",
    "opt = torch.optim.Adam(ae.parameters(), lr=3e-4)\n",
    "\n",
    "for i in tqdm(range(25)):\n",
    "    loss_rec = train_loop(val_loader, ae, opt, device)\n",
    "    print('loss from epoch ', i, ': ', loss_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae.eval()\n",
    "image, label = val_dataset[737]\n",
    "image = rearrange(image.to(device), 'b h w -> b (h w)')\n",
    "pred = ae(image)\n",
    "pred = rearrange(pred, 'b (h w) -> b h w', h=28, w=28)\n",
    "image = rearrange(image, 'b (h w) -> b h w', h=28, w=28)\n",
    "\n",
    "fig, (ax1, ax3) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax1.imshow(image.detach().cpu().squeeze().numpy())\n",
    "ax3.imshow(pred.detach().cpu().squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
